{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "C192SOmJS6lw",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# CS 195: Natural Language Processing\n",
    "## Subword Tokenization and Byte Pair Encoding\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/s26-CS195NLP/blob/main/F2_4_SubwordTokenizationBPE.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Wrapping the output in Colab\n",
    "\n",
    "Here's a resource on how to get Google Colab to wrap the output of a cell: https://stackoverflow.com/questions/58890109/line-wrapping-in-collaboratory-google-results\n",
    "\n",
    "In short, put the following into a cell and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Announcement for next Monday\n",
    "\n",
    "**First half of class:** NLP at Berkshire Hathaway Energy\n",
    "* Vinay Kodigante\n",
    "* Ryan Blokhuis (Drake alum)\n",
    "* Amanda Pereira (Drake alum)\n",
    "\n",
    "**Second half of class:** Normal demo day\n",
    "* Does anyone have a *creative synthesis* project they're planning to show off that they'd like show our visitors?\n",
    "\n",
    "**Before and during:** University Marketing and Communications may take some photos/videos for a social media post\n",
    "\n",
    "<img src=\"images/JasonBHE.jpg\"/>\n",
    "New alumni hire: Jason Nguyen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    "* [GPT Tokenizer Illustration](https://platform.openai.com/tokenizer)\n",
    "* [Hugging Face Byte-Pair Encoding tokenization](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt)\n",
    "* [Chapter 2: Words and Tokens](https://web.stanford.edu/~jurafsky/slp3/2.pdf). *Speech and Language Processing.* Daniel Jurafsky & James H. Martin\n",
    "* Python resources for retrieving and tokenizing text data:\n",
    "    * [Python `requests` library quickstart](https://requests.readthedocs.io/en/latest/user/quickstart/)\n",
    "    * [Beautiful Soup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "    * [Python `split` method](https://docs.python.org/3/library/stdtypes.html#str.split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#uncomment if you need to install things we're using today\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install requests transformers beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## What is a token?\n",
    "\n",
    "A **token** is the unit of data that a language model operates on\n",
    "* In our previous Markov Model, each word or punctuation mark counted as a token\n",
    "* Tokens can also be parts of words (i.e., subwords) or even individual characters\n",
    "\n",
    "**Tokenization** is the process of converting a stream of raw text into tokens - grouping it into groups of characters that represent a coherent unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Why do subword tokenization?\n",
    "\n",
    "Natural langauge has a huge number of words (english has hundreds of thousands) and most are not frequently used\n",
    "\n",
    "There common prefixes and suffixes, etc. that modify words in reliable ways\n",
    "\n",
    "* think of a word like \"hyperparameterization\" which is \"hyper\" + \"parameter\" + \"iz\" + \"ation\"\n",
    "* or \"hyperfixation\" = \"hyper\" + \"fix\" + \"ation\"\n",
    "* or \"organization\" = \"oran\" + \"iz\" + \"ation\"\n",
    "\n",
    "Mixtures of tokens gives the necessary meaning without having to create new tokens for every single variation\n",
    "\n",
    "A model might be able to handle \"hyperparameterization\" without that word appearing in its training corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "### Group Exercise\n",
    "\n",
    "Try writing some sentences in the OpenAI tokenizer illustration: https://platform.openai.com/tokenizer\n",
    "\n",
    "Discuss any interesting things you notice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Unpacking the pipeline a little\n",
    "\n",
    "Go to a Hugging Face model card and click the `Use this model` button near the upper-right part of the page\n",
    "\n",
    "Example: https://huggingface.co/HuggingFaceTB/SmolLM2-135M\n",
    "\n",
    "Notice that it separates the tokenizer from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562b565b9b2e4f159cca7f2cf1b4b233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea7250fe2914e53be15950b7f41cd15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0eb6b6261b4258963698677dbc88b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26e0446bc29431ca7bbe948165a48b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c24d6dbbfa40b8a9178d9fe8b4849a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00ef712e2ec433dbb51ffa675fb637b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/704 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309af154b5ba4b3fa67a4c7cf3de7e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98955c6047b4a1db20b043dff6c9882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "## Using the tokenizer by itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [4239, 506, 1576, 288, 1044, 638, 3511, 308, 34519, 34, 9624, 3215, 624, 9577, 30], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer(\"Let's try to understand how SmolLM2 tokenizes its inputs.\")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Discussion\n",
    "\n",
    "What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let',\n",
       " \"'s\",\n",
       " 'Ġtry',\n",
       " 'Ġto',\n",
       " 'Ġunderstand',\n",
       " 'Ġhow',\n",
       " 'ĠSm',\n",
       " 'ol',\n",
       " 'LM',\n",
       " '2',\n",
       " 'Ġtoken',\n",
       " 'izes',\n",
       " 'Ġits',\n",
       " 'Ġinputs',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenized_text['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Now what do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "## Let's investigate the same word written in different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [4239, 506, 2959, 506, 1303, 506, 198, 4239, 506], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['Let', \"'s\", 'ĠLet', \"'s\", 'Ġlet', \"'s\", 'Ċ', 'Let', \"'s\"]\n"
     ]
    }
   ],
   "source": [
    "more_experiments = tokenizer(\"Let's Let's let's\\nLet's\")\n",
    "print(more_experiments)\n",
    "print(tokenizer.convert_ids_to_tokens(more_experiments['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Now what do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## How do we use the tokenized data with the model object?\n",
    "\n",
    "Underneath the `transformers` module is a lot of PyTorch code, which uses the `tensor` data type. \n",
    "\n",
    "A **tensor** is a multidimensional array, often representing a mathematical vector or matrix\n",
    "\n",
    "Note that this is **not** the *instruct* version of the SmolLM2 model, so we can use it for text completion.\n",
    "\n",
    "There's also a `decode` function that does all the work of converting the list of tokens back into readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'tensor' version of the tokens:\n",
      " {'input_ids': tensor([[ 4239,   506,  1576,   288,  1044,   638,  3511,   308, 34519,  9624,\n",
      "          3215,   624]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "The 'tensor' returned by the model:\n",
      " tensor([[ 4239,   506,  1576,   288,  1044,   638,  3511,   308, 34519,  9624,\n",
      "          3215,   624,   940,    30,   198,   198,  9207,   308, 34519,  9624,\n",
      "          3215,   624,   940,   411,  1015,   253, 15345,  1517,    30,   378,\n",
      "         15345,  1517]])\n",
      "Using convert_ids_to_tokens:\n",
      " ['Let', \"'s\", 'Ġtry', 'Ġto', 'Ġunderstand', 'Ġhow', 'ĠSm', 'ol', 'LM', 'Ġtoken', 'izes', 'Ġits', 'Ġdata', '.', 'Ċ', 'Ċ', 'Sm', 'ol', 'LM', 'Ġtoken', 'izes', 'Ġits', 'Ġdata', 'Ġby', 'Ġusing', 'Ġa', 'Ġhash', 'Ġfunction', '.', 'ĠThe', 'Ġhash', 'Ġfunction']\n",
      "Using decode:\n",
      " Let's try to understand how SmolLM tokenizes its data.\n",
      "\n",
      "SmolLM tokenizes its data by using a hash function. The hash function\n"
     ]
    }
   ],
   "source": [
    "partial_sentence = \"Let's try to understand how SmolLM tokenizes its\"\n",
    "partial_sentence_tokenized = tokenizer(partial_sentence, return_tensors=\"pt\")\n",
    "print(\"The 'tensor' version of the tokens:\\n\", partial_sentence_tokenized)\n",
    "model_response = model.generate(\n",
    "    input_ids = partial_sentence_tokenized[\"input_ids\"], \n",
    "    attention_mask = partial_sentence_tokenized[\"attention_mask\"]\n",
    ")\n",
    "print(\"The 'tensor' returned by the model:\\n\",model_response)\n",
    "print(\"Using convert_ids_to_tokens:\\n\",tokenizer.convert_ids_to_tokens(model_response[0]))\n",
    "print(\"Using decode:\\n\",tokenizer.decode(model_response[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Normalization and Pre-Tokenization\n",
    "\n",
    "Text data often goes through a **normalization** step before tokenization\n",
    "* removing extra whitespace\n",
    "* converting to lowercase\n",
    "* replace accented characters with unaccented (e.g., replace á with a)\n",
    "\n",
    "**Pre-tokenization**\n",
    "* separate characters into groups based on whitespace (and maybe punctuation)\n",
    "* like when we used `split()` for our MarkovModel data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Byte-Pair Encoding (BPE)\n",
    "\n",
    "BPE is a tokenization algorithm\n",
    "* Probably the most commonly used one (GPT, SmolLM, and many others)\n",
    "\n",
    "BPE Training Algorithm Idea:\n",
    "1. Pretokenize all of the things separated by whitespace, etc.\n",
    "2. Start with a *vocabulary* containing all the individual characters from the pre-tokens\n",
    "3. Until the vocabulary is the desired size\n",
    "    * merge together the most frequently-appearing pair of tokens and add it as a new token to the *vocabulary*\n",
    "  \n",
    "Frequent words - don't break them apart\n",
    "\n",
    "Less-frequent words - represent them as several subwords\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## BPE Training Example\n",
    "\n",
    "Let's go through the example from here: https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt\n",
    "\n",
    "Assume the corpus has only the following words: \"hug\", \"pug\", \"pun\", \"bun\", \"hugs\" and they appear with these frequencies (meaning \"hug\" appears 10 times, \"pug\" appears 5 times, etc:\n",
    "\n",
    "(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The initial vocabulary will be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocabulary = [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Tokenize our pre-tokens using this vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus = [(\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Find how frequently each pair appears.\n",
    "\n",
    "* \"h\" \"u\" appears 15 times (10 from \"hug\", 5 from \"hugs\")\n",
    "* \"u\" \"g\" appears 20 times (10 from \"hug\", 5 from \"pug\", 5 from \"hugs\")\n",
    "* \"p\" \"u\" appears __ times (exercise)\n",
    "* \"u\" \"n\" appears __ times (exercise)\n",
    "* \"b\" \"u\" appears __ times (exercise)\n",
    "* \"g\" \"s\" appears __ times (exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "If \"u\" \"g\" is the most frequent, we then add it to our vocabulary. \n",
    "\n",
    "Vocabulary and corpus are now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocabulary = [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\"]\n",
    "corpus = [(\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Finding pair frequencies\n",
    "\n",
    "* \"h\" \"ug\" appears 15 times (10 from \"hug\", 5 from \"hugs\")\n",
    "* \"p\" \"ug\" appears 5 times\n",
    "* \"p\" \"u\"  appears 12 times\n",
    "* \"u\" \"n\"  appears 16 times\n",
    "* \"b\" \"u\"  appears 4 times\n",
    "* \"ug\" \"s\" appears 5 times\n",
    "\n",
    "So we merge \"u\" \"n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocabulary = [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\"]\n",
    "corpus = [(\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"h\" \"ug\" \"s\", 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Finding pair frequencies\n",
    "\n",
    "* \"h\" \"ug\" appears 15 times\n",
    "* \"p\" \"ug\" appears 5 times\n",
    "* \"p\" \"un\"  appears 12 times\n",
    "* \"b\" \"un\"  appears 4 times\n",
    "* \"ug\" \"s\" appears 5 times\n",
    "\n",
    "So we merge \"h\" \"ug\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocabulary = [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]\n",
    "corpus = [(\"hug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"hug\" \"s\", 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Exercise:** when happens on the next merge step?\n",
    "\n",
    "Remember - we stop when the vocabular hits a pre-determined size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Tokenizing with BPE\n",
    "\n",
    "The algorithm for tokenizing with this trained model (vocabulary) is\n",
    "\n",
    "1. pre-tokenize (split by spaces)\n",
    "2. split pre-tokens (words) into characters, use \"[UNK]\" for unknowns\n",
    "3. apply the merge rules that were learned *in order*\n",
    "\n",
    "In our example, we learned the rules\n",
    "* (\"u\", \"g\") -> \"ug\"\n",
    "* (\"u\", \"n\") -> \"un\"\n",
    "* (\"h\", \"ug\") -> \"hug\"\n",
    "\n",
    "Example text: \"pun bug hugs mug\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocabulary = [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]\n",
    "pre_tokens = [\"pun\",\"bug\",\"hugs\",\"mug\"]\n",
    "split_pre_tokens = [[\"p\",\"u\",\"n\"],[\"b\",\"u\",\"g\"],[\"h\",\"u\",\"g\",\"s\"],[\"m\",\"u\",\"g\"]]\n",
    "split_pre_tokens = [[\"p\",\"u\",\"n\"],[\"b\",\"u\",\"g\"],[\"h\",\"u\",\"g\",\"s\"],[\"[UNK]\",\"u\",\"g\"]] #there is no m in the vocabulary\n",
    "after_first_merge_rule = [[\"p\",\"u\",\"n\"],[\"b\",\"ug\"],[\"h\",\"ug\",\"s\"],[\"[UNK]\",\"ug\"]]\n",
    "after_second_merge_rule = [[\"p\",\"un\"],[\"b\",\"ug\"],[\"h\",\"ug\",\"s\"],[\"[UNK]\",\"ug\"]]\n",
    "after_third_merge_rule = [[\"p\",\"un\"],[\"b\",\"ug\"],[\"hug\",\"s\"],[\"[UNK]\",\"ug\"]] \n",
    "final_tokens = [\"p\",\"un\",\"b\",\"ug\",\"hug\",\"s\",\"[UNK]\",\"ug\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Usually any character from the encoding system (UTF-8, Unicode, etc.) can be included in the vocabulary, but you might occasionally run into something non-standard and need to use `[UNK]`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Applied Exploration\n",
    "\n",
    "Find some new text, tokenize it according to one or more of the methods discussed here\n",
    "\n",
    "Use it as input for the Markov Chain in the previous set of notes\n",
    "\n",
    "Describe what you did and record notes about your results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Extended Implementation Idea\n",
    "\n",
    "Write the code that will implement a tokenization algorithm like BPE or WordPiece automatically\n",
    "\n",
    "Both the BPE and WordPiece have examples in the Hugging Face course\n",
    "* https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt\n",
    "* https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt\n",
    "\n",
    "Except: they use Hugging Face tokenizers for the pre-tokenization. You should do it using `split()` like we did. Because of the way that it groups tokens, I don't think you should have to worry about separating out punctuation, just include it in your vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Working with text you find on the web\n",
    "\n",
    "Here's an example of using the `requests` library to load data from a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġactivity', ',', 'Ġhowever', ',', 'Ġwhich', 'ĠI', 'Ġmerely', 'Ġshared', 'Ġwith', 'Ġall', 'Ġthe', 'Ġreaders', 'Ġof', 'č', 'Ċ', 'the', 'Ġdaily', 'Ġpress', ',', 'ĠI', 'Ġknew', 'Ġlittle', 'Ġof', 'Ġmy', 'Ġformer', 'Ġfriend', 'Ġand', 'Ġcompanion', '.', 'čĊč', 'Ċ', 'One', 'Ġnight', 'âĢĶ', 'it', 'Ġwas', 'Ġon', 'Ġthe', 'Ġtwentieth', 'Ġof', 'ĠMarch', ',', 'Ġ', '1', '8', '8', '8', 'âĢĶ', 'I', 'Ġwas', 'Ġreturning', 'Ġfrom', 'Ġa', 'č', 'Ċ', 'jour', 'ney', 'Ġto', 'Ġa', 'Ġpatient', 'Ġ(', 'for', 'ĠI', 'Ġhad', 'Ġnow', 'Ġreturned', 'Ġto', 'Ġcivil', 'Ġpractice', '),', 'Ġwhen', 'č', 'Ċ', 'my', 'Ġway', 'Ġled', 'Ġme', 'Ġthrough', 'ĠBaker', 'ĠStreet', '.', 'ĠAs', 'ĠI', 'Ġpassed', 'Ġthe', 'Ġwell', '-', 'rem', 'embered', 'č', 'Ċ', 'door', ',', 'Ġwhich', 'Ġmust', 'Ġalways', 'Ġbe', 'Ġassociated', 'Ġin', 'Ġmy']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "response = requests.get(\"https://www.gutenberg.org/files/1661/1661-0.txt\")\n",
    "sherlock_raw_text = response.text\n",
    "\n",
    "sherlock_tokens = tokenizer.tokenize( sherlock_raw_text )\n",
    "print(sherlock_tokens[1000:1100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "If the text is in `html` instead of plaintext, you can try to parse it out with the beautiful soup package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d', 'Ġbi', 'ographer', ',', 'ĠDr', '.', 'ĠJohn', 'ĠH', '.', 'ĠWatson', ',', 'Ġwho', 'Ġusually', 'Ġaccompanies', 'ĠHolmes', 'Ġduring', 'Ġhis', 'Ġinvestigations', 'Ġand', 'Ġoften', 'Ġshares', 'Ġquarters', 'Ġwith', 'Ġhim', 'Ġat', 'Ġthe', 'Ġaddress', 'Ġof', 'Ġ', '2', '2', '1', 'B', 'ĠBaker', 'ĠStreet', ',', 'ĠLondon', ',', 'Ġwhere', 'Ġmany', 'Ġof', 'Ġthe', 'Ġstories', 'Ġbegin', '.', 'Ċ', 'Though', 'Ġnot', 'Ġthe', 'Ġfirst', 'Ġfictional', 'Ġdetective', ',', 'ĠSher', 'lock', 'ĠHolmes', 'Ġis', 'Ġarguably', 'Ġthe', 'Ġbest', 'Ġknown', '.[', '1', ']', 'ĠBy', 'Ġthe', 'Ġ', '1', '9', '9', '0', 's', ',', 'Ġover', 'Ġ', '2', '5', ',', '0', '0', '0', 'Ġstage', 'Ġadaptations', ',', 'Ġfilms', ',', 'Ġtelevision', 'Ġproductions', ',', 'Ġand', 'Ġpublications', 'Ġhad', 'Ġfeatured', 'Ġthe', 'Ġdetective', ',[', '2', ']', 'Ġand', 'ĠGuin', 'ness', 'ĠWorld', 'ĠRecords', 'Ġlists', 'Ġhim', 'Ġas', 'Ġthe', 'Ġmost', 'Ġportrayed', 'Ġhuman', 'Ġliterary', 'Ġcharacter']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# wikipedia requires you to set a User Agent if you want to access the page in code\n",
    "headers = {\n",
    "    'User-Agent': 'DrakeNLPClassScraper/1.0.0'\n",
    "}\n",
    "\n",
    "response = requests.get(\"https://en.wikipedia.org/wiki/Sherlock_Holmes\", headers=headers)\n",
    "sherlock_wiki_html = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "sherlock_wiki_text = sherlock_wiki_html.get_text()\n",
    "\n",
    "sherlock_wiki_tokens = tokenizer.tokenize( sherlock_wiki_text[5000:5500] )\n",
    "\n",
    "\n",
    "print(sherlock_wiki_tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "authorship_tag": "ABX9TyOf2oi4GbgdvkO0orSdgZtQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
